{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64757af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1851718e",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce9e19c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc9ac9d4b0d84e649b8fe7569edb10ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/844k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf8c9a895fb46d88a5a84a5c4a83b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf3f57d391714feeba067415538df1dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/486 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cc = load_dataset('HuggingFaceFW/fineweb-2', 'kor_Hang', cache_dir='../cache', split='train')\n",
    "remove_columns = [i for i in cc.column_names if i!='text']\n",
    "cc = cc.remove_columns(remove_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "448240cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import concatenate_datasets\n",
    "from datasets import DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "664c4768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "907bcbdc8dd04704870fe0ab86f2ec3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cc_2 = load_dataset('HuggingFaceFW/fineweb-2', 'kor_Hang', cache_dir='../cache', split='test')\n",
    "remove_columns = [i for i in cc_2.column_names if i!='text']\n",
    "cc_2 = cc_2.remove_columns(remove_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f3d0969",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = concatenate_datasets([cc,cc_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "310df43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_path = '/home/work/g-earth-22/ocw/total/ko'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47a18842",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = {}\n",
    "dataset_dict['cc']=cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87c14123",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a19cffc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22% 2/9 [00:02<00:07,  1.13s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe15b76f653e44dba0225368edc2f3c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e18bd7d37a84c4ea739f2c32d685654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78% 7/9 [00:15<00:03,  1.68s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d05e7f284ea84d16b816e12cc3e96e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89% 8/9 [00:18<00:02,  2.16s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26e3d6cf180e49b1baf39dbd42c75e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 9/9 [00:20<00:00,  2.26s/it]\n"
     ]
    }
   ],
   "source": [
    "for name in tqdm(os.listdir(total_data_path)):\n",
    "    path = os.path.join(total_data_path, name)\n",
    "    datasets = []\n",
    "    if os.path.isdir(path):\n",
    "        for data_name in os.listdir(path):\n",
    "            try:\n",
    "                d = load_dataset('json', data_files=os.path.join(path, data_name), split='train')\n",
    "            except:\n",
    "                d = load_dataset('json', data_files=os.path.join(path, data_name))\n",
    "            remove_columns = [i for i in d.column_names if i!='text']\n",
    "            d = d.remove_columns(remove_columns)\n",
    "            datasets.append(d)\n",
    "    datasets = concatenate_datasets(datasets)\n",
    "    dataset_dict[name]=datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f359ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 10/10 [03:29<00:00, 20.97s/it]\n"
     ]
    }
   ],
   "source": [
    "_dataset_dict = dict()\n",
    "for name, ds in tqdm(dataset_dict.items()):\n",
    "    ds = ds.add_column('src',[name]*len(ds))\n",
    "    _dataset_dict[name]=ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94e69f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cc = cc.add_column('src',['cc']*len(cc))\n",
    "# _dataset_dict['cc']=cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "086d2e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tiktoken\n",
    "import base64\n",
    "\n",
    "def load_tiktoken_tokenizer(file_path):\n",
    "    from tiktoken.load import load_tiktoken_bpe\n",
    "    with open(file_path, 'r') as f:\n",
    "        tiktoken_json = json.loads(f.read())\n",
    "    mergeable_ranks = {\n",
    "      base64.b64decode(token): int(rank)\n",
    "      for token, rank in (line.split() for line in tiktoken_json[\"mergeable_ranks\"].splitlines() if line)\n",
    "    }\n",
    "    tokenizer = tiktoken.Encoding(\n",
    "      name=file_path.split('/')[-1],\n",
    "      pat_str=tiktoken_json['pat_str'],\n",
    "      mergeable_ranks=mergeable_ranks,\n",
    "      special_tokens=tiktoken_json['special_tokens']\n",
    "    )\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "314204f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, and if the total_length < block_size  we exclude this batch and return an empty dict.\n",
    "    # We could add padding if the model supported it instead of this drop, you can customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size        \n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8dee01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_eos_token(text_list, eos_token='<|end_of_text|>'):\n",
    "    output = []\n",
    "    for i in text_list:\n",
    "        output.append(i+eos_token)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75c46ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "def train_test_split(ds, num=500):\n",
    "    ds = ds.shuffle(seed=42)\n",
    "    validation = ds.select(range(num))\n",
    "    train = ds.select(range(num,len(ds)))\n",
    "    return train, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5b931d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOAD TOKENIZER\n",
    "# ex) tokenizer = load_tiktoken_tokenizer('kt_tokenizer_v3/tiktoken/kt_v3.tiktoken')\n",
    "TOKENIZER_MODEL_PATH='/home/work/g-earth-22/ocw/tokenizer/tiktoken/kt_v3.tiktoken'\n",
    "tokenizer = load_tiktoken_tokenizer(f'{TOKENIZER_MODEL_PATH}')\n",
    "\n",
    "tokenizer.encode('<|end_of_text|>', allowed_special='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84a75d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03fabb8",
   "metadata": {},
   "source": [
    "# train-test dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7aa00e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '../data/data_mix/original'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "afb7eb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f6f6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cc\n"
     ]
    }
   ],
   "source": [
    "for name, ds in tqdm(_dataset_dict.items()):\n",
    "    print(name)\n",
    "    train, validation = train_test_split(_dataset_dict[name])\n",
    "    ds_dict = DatasetDict({'train':train,'validation':validation})\n",
    "    ds_dict.save_to_disk(os.path.join(save_path, name), num_proc=64)\n",
    "    print('saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d6474f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cc\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80f75913c88149c69533d67ff32ff3da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=64):   0%|          | 0/58188374 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for name, ds in tqdm(_dataset_dict.items()):\n",
    "    print(name)\n",
    "    train, validation = train_test_split(_dataset_dict[name])\n",
    "    ds_dict = DatasetDict({'train':train,'validation':validation})\n",
    "    ds_dict = ds_dict.map(\n",
    "        lambda x: {\"input_ids\": tokenizer.encode_batch(add_eos_token(x),allowed_special='all')},\n",
    "        input_columns='text',\n",
    "        batched=True,\n",
    "        num_proc=64)\n",
    "    column_names = [i for i in ds_dict['train'].column_names if i not in ['input_ids']]\n",
    "    _ds_dict = ds_dict.remove_columns(column_names)\n",
    "    lm_datasets = _ds_dict.map(\n",
    "            group_texts,\n",
    "            batched=True,\n",
    "            num_proc=64,\n",
    "            desc=f\"Grouping texts in chunks of {block_size}\",\n",
    "        )\n",
    "    lm_datasets.save_to_disk(os.path.join(save_path, name), num_proc=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.4 (NGC 24.07/Python 3.10) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
