{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e919a75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef9c2b1",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6b54d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc9ac9d4b0d84e649b8fe7569edb10ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/844k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf8c9a895fb46d88a5a84a5c4a83b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf3f57d391714feeba067415538df1dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/486 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cc = load_dataset('HuggingFaceFW/fineweb-2', 'kor_Hang', cache_dir='../cache', split='train')\n",
    "remove_columns = [i for i in cc.column_names if i!='text']\n",
    "cc = cc.remove_columns(remove_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b55af52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import concatenate_datasets\n",
    "from datasets import DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f57e9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "907bcbdc8dd04704870fe0ab86f2ec3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cc_2 = load_dataset('HuggingFaceFW/fineweb-2', 'kor_Hang', cache_dir='../cache', split='test')\n",
    "remove_columns = [i for i in cc_2.column_names if i!='text']\n",
    "cc_2 = cc_2.remove_columns(remove_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c75b825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = concatenate_datasets([cc,cc_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53416854",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_path = '/home/work/g-earth-22/ocw/total/ko'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbe48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = {}\n",
    "# dataset_dict['cc']=cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30f07e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f25ad04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22% 2/9 [00:01<00:04,  1.41it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc138e8d9474f4da48fd73c866ea7fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "098f24fcfc0d4983b233681a5d58ac08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78% 7/9 [00:11<00:02,  1.20s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39384fbaa4446e0a482a8ca4310bf17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89% 8/9 [00:14<00:01,  1.56s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8c1dae735d84808859ec9f27b1559e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 9/9 [00:15<00:00,  1.70s/it]\n"
     ]
    }
   ],
   "source": [
    "for name in tqdm(os.listdir(total_data_path)):\n",
    "    path = os.path.join(total_data_path, name)\n",
    "    datasets = []\n",
    "    if os.path.isdir(path):\n",
    "        for data_name in os.listdir(path):\n",
    "            try:\n",
    "                d = load_dataset('json', data_files=os.path.join(path, data_name), split='train')\n",
    "            except:\n",
    "                d = load_dataset('json', data_files=os.path.join(path, data_name))\n",
    "            remove_columns = [i for i in d.column_names if i!='text']\n",
    "            d = d.remove_columns(remove_columns)\n",
    "            datasets.append(d)\n",
    "    datasets = concatenate_datasets(datasets)\n",
    "    dataset_dict[name]=datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6873f9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 9/9 [00:57<00:00,  6.41s/it]\n"
     ]
    }
   ],
   "source": [
    "_dataset_dict = dict()\n",
    "for name, ds in tqdm(dataset_dict.items()):\n",
    "    ds = ds.add_column('src',[name]*len(ds))\n",
    "    _dataset_dict[name]=ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ce0fa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cc = cc.add_column('src',['cc']*len(cc))\n",
    "# _dataset_dict['cc']=cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ca408a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tiktoken\n",
    "import base64\n",
    "\n",
    "def load_tiktoken_tokenizer(file_path):\n",
    "    from tiktoken.load import load_tiktoken_bpe\n",
    "    with open(file_path, 'r') as f:\n",
    "        tiktoken_json = json.loads(f.read())\n",
    "    mergeable_ranks = {\n",
    "      base64.b64decode(token): int(rank)\n",
    "      for token, rank in (line.split() for line in tiktoken_json[\"mergeable_ranks\"].splitlines() if line)\n",
    "    }\n",
    "    tokenizer = tiktoken.Encoding(\n",
    "      name=file_path.split('/')[-1],\n",
    "      pat_str=tiktoken_json['pat_str'],\n",
    "      mergeable_ranks=mergeable_ranks,\n",
    "      special_tokens=tiktoken_json['special_tokens']\n",
    "    )\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22686d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, and if the total_length < block_size  we exclude this batch and return an empty dict.\n",
    "    # We could add padding if the model supported it instead of this drop, you can customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size        \n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6488c924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_eos_token(text_list, eos_token='<|end_of_text|>'):\n",
    "    output = []\n",
    "    for i in text_list:\n",
    "        output.append(i+eos_token)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "948d3202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "def train_test_split(ds, num=500):\n",
    "    ds = ds.shuffle(seed=42)\n",
    "    validation = ds.select(range(num))\n",
    "    train = ds.select(range(num,len(ds)))\n",
    "    return train, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "876f12c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOAD TOKENIZER\n",
    "# ex) tokenizer = load_tiktoken_tokenizer('kt_tokenizer_v3/tiktoken/kt_v3.tiktoken')\n",
    "TOKENIZER_MODEL_PATH='/home/work/g-earth-22/ocw/tokenizer/tiktoken/kt_v3.tiktoken'\n",
    "tokenizer = load_tiktoken_tokenizer(f'{TOKENIZER_MODEL_PATH}')\n",
    "\n",
    "tokenizer.encode('<|end_of_text|>', allowed_special='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bb3e33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700b3df4",
   "metadata": {},
   "source": [
    "# train-test dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6530967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '../data/data_mix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d4bd1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7807f896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, ds in tqdm(_dataset_dict.items()):\n",
    "#     print(name)\n",
    "#     train, validation = train_test_split(_dataset_dict[name])\n",
    "#     ds_dict = DatasetDict({'train':train,'validation':validation})\n",
    "#     ds_dict.save_to_disk(os.path.join(save_path, name), num_proc=64)\n",
    "#     print('saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6674a6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encyclo\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "396a4aad86e24c69a8476cc9f96b78b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=64):   0%|          | 0/694015 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6a36a8925249d4a5a7746a685da723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=64):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c27cc3fffaf4c65b5bcae842cb2a2e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 2048 (num_proc=64):   0%|          | 0/694015 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954c4c5bc15e4b5f9654135269edffc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 2048 (num_proc=64):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa2630f52af649da842292d5c0a2ef3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/64 shards):   0%|          | 0/200220 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ae42afa64c4e22b1c0b2856e244ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/64 shards):   0%|          | 0/113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11% 1/9 [01:04<08:38, 64.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosmopedia\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883e3c3a10ed4799a022e5afeb989a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=64):   0%|          | 0/2149493 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for name, ds in tqdm(_dataset_dict.items()):\n",
    "    print(name)\n",
    "    train, validation = train_test_split(_dataset_dict[name])\n",
    "    ds_dict = DatasetDict({'train':train,'validation':validation})\n",
    "    ds_dict = ds_dict.map(\n",
    "        lambda x: {\"input_ids\": tokenizer.encode_batch(add_eos_token(x),allowed_special='all')},\n",
    "        input_columns='text',\n",
    "        batched=True,\n",
    "        num_proc=64)\n",
    "    column_names = [i for i in ds_dict['train'].column_names if i not in ['input_ids']]\n",
    "    _ds_dict = ds_dict.remove_columns(column_names)\n",
    "    lm_datasets = _ds_dict.map(\n",
    "            group_texts,\n",
    "            batched=True,\n",
    "            num_proc=64,\n",
    "            desc=f\"Grouping texts in chunks of {block_size}\",\n",
    "        )\n",
    "    lm_datasets.save_to_disk(os.path.join(save_path, name), num_proc=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.4 (NGC 24.07/Python 3.10) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
